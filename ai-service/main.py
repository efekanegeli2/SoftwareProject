from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional

app = FastAPI(title="AI Scoring Service", version="1.0.0")

# CORS configuration
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # In production, specify exact origins
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Request/Response models
class ScoreRequest(BaseModel):
    mcq_answers: List[str] = []
    writing_text: str = ""
    speaking_text: str = ""
    speaking_score: Optional[int] = None  # Frontend-calculated speaking score

class QuestionRequest(BaseModel):
    topic: str = "grammar"
    difficulty: str = "intermediate"  # beginner, intermediate, advanced
    count: int = 1

class GeneratedQuestion(BaseModel):
    text: str
    options: List[str]
    correct: str
    explanation: str

class ScoreResponse(BaseModel):
    overall_score: int
    cefr_level: str
    feedback: str
    individual_scores: dict = {}  # mcq_score, writing_score, speaking_score

# Hardcoded answer key for MCQ (20 questions)
# Matches the questions generated by the server
MCQ_ANSWER_KEY = [
    "went",           # Q1: She ___ to the store yesterday
    "on",             # Q2: working ___ this project
    "The",            # Q3: ___ sun rises
    "more interesting", # Q4: comparative form
    "had studied",    # Q5: conditional
    "is read",        # Q6: passive voice
    "should",         # Q7: modal verb
    "whom",           # Q8: relative pronoun
    "will have finished", # Q9: future perfect
    "have lived",     # Q10: present perfect
    "went",           # Q11: (repeat)
    "on",             # Q12: (repeat)
    "The",            # Q13: (repeat)
    "more interesting", # Q14: (repeat)
    "had studied",    # Q15: (repeat)
    "is read",        # Q16: (repeat)
    "should",         # Q17: (repeat)
    "whom",           # Q18: (repeat)
    "will have finished", # Q19: (repeat)
    "have lived"      # Q20: (repeat)
]

def calculate_mcq_score(answers: List[str]) -> int:
    """Calculate MCQ score based on answer key."""
    if not answers or len(answers) == 0:
        return 0
    
    correct = 0
    for i, answer in enumerate(answers):
        if i < len(MCQ_ANSWER_KEY) and answer == MCQ_ANSWER_KEY[i]:
            correct += 1
    
    # Score out of 100 (each question worth 5 points)
    return min(100, (correct * 5))

def calculate_writing_score(text: str) -> int:
    """Mock writing score based on length and vocabulary complexity."""
    if not text or len(text.strip()) == 0:
        return 0
    
    word_count = len(text.split())
    char_count = len(text)
    
    # Base score on length
    length_score = min(50, (word_count * 2))
    
    # Mock vocabulary complexity (check for advanced words)
    advanced_words = ["artificial", "intelligence", "technology", "innovation", 
                     "revolutionary", "transform", "efficiency", "automation",
                     "sophisticated", "capabilities", "enhancement", "optimization"]
    
    complexity_score = 0
    text_lower = text.lower()
    for word in advanced_words:
        if word in text_lower:
            complexity_score += 5
    
    complexity_score = min(50, complexity_score)
    
    return min(100, length_score + complexity_score)

def calculate_speaking_score(text: str, frontend_score: int = None) -> int:
    """Calculate speaking score based on frontend transcription analysis."""
    import json
    import time

    # Hypothesis D: AI service speaking score calculation entry
    log_entry = {
        "id": f"log_{int(time.time()*1000)}_ai_entry",
        "timestamp": int(time.time()*1000),
        "location": "ai-service/main.py:104",
        "message": "AI calculate_speaking_score called",
        "data": {"frontend_score": frontend_score, "text_length": len(text) if text else 0},
        "sessionId": "debug-session",
        "runId": "initial",
        "hypothesisId": "D"
    }

    with open('c:\\Users\\Burkay Çakar\\Desktop\\SoftwareProject\\.cursor\\debug.log', 'a') as f:
        f.write(json.dumps(log_entry) + '\n')

    # Use frontend score if provided (more accurate word-matching algorithm)
    if frontend_score is not None:
        final_score = min(100, frontend_score * 5)  # Convert 0-20 scale to 0-100

        # Hypothesis D: Frontend score used
        log_entry = {
            "id": f"log_{int(time.time()*1000)}_ai_frontend",
            "timestamp": int(time.time()*1000),
            "location": "ai-service/main.py:118",
            "message": "Frontend score used for speaking",
            "data": {"frontend_score": frontend_score, "final_score": final_score},
            "sessionId": "debug-session",
            "runId": "initial",
            "hypothesisId": "D"
        }

        with open('c:\\Users\\Burkay Çakar\\Desktop\\SoftwareProject\\.cursor\\debug.log', 'a') as f:
            f.write(json.dumps(log_entry) + '\n')

        return final_score

    # Fallback logic for when frontend score is not available
    if not text or len(text.strip()) == 0:
        # Hypothesis D: Fallback - empty text
        log_entry = {
            "id": f"log_{int(time.time()*1000)}_ai_empty",
            "timestamp": int(time.time()*1000),
            "location": "ai-service/main.py:136",
            "message": "Fallback - empty text",
            "data": {"return_score": 0},
            "sessionId": "debug-session",
            "runId": "initial",
            "hypothesisId": "D"
        }

        with open('c:\\Users\\Burkay Çakar\\Desktop\\SoftwareProject\\.cursor\\debug.log', 'a') as f:
            f.write(json.dumps(log_entry) + '\n')

        return 0

    # Basic validation: check if text contains recognizable English words
    words = text.lower().split()
    english_words = ['the', 'and', 'is', 'in', 'to', 'of', 'a', 'that', 'it', 'with', 'as', 'for', 'was', 'on', 'are', 'be', 'this', 'have', 'or', 'by']

    # Count recognized English words
    recognized_words = sum(1 for word in words if any(ew in word for ew in english_words) or len(word) > 3)

    # Calculate score based on word count and recognition
    word_score = min(60, len(words) * 3)
    recognition_score = min(40, recognized_words * 4)
    final_score = min(100, word_score + recognition_score)

    # Hypothesis D: Fallback calculation complete
    log_entry = {
        "id": f"log_{int(time.time()*1000)}_ai_fallback",
        "timestamp": int(time.time()*1000),
        "location": "ai-service/main.py:165",
        "message": "Fallback speaking score calculated",
        "data": {"word_score": word_score, "recognition_score": recognition_score, "final_score": final_score},
        "sessionId": "debug-session",
        "runId": "initial",
        "hypothesisId": "D"
    }

    with open('c:\\Users\\Burkay Çakar\\Desktop\\SoftwareProject\\.cursor\\debug.log', 'a') as f:
        f.write(json.dumps(log_entry) + '\n')

    return final_score

def determine_cefr_level(score: int) -> str:
    """Determine CEFR level based on overall score."""
    if score >= 90:
        return "C2"
    elif score >= 80:
        return "C1"
    elif score >= 70:
        return "B2"
    elif score >= 60:
        return "B1"
    elif score >= 40:
        return "A2"
    else:
        return "A1"

def generate_feedback(score: int, cefr_level: str) -> str:
    """Generate feedback based on score and level."""
    if score >= 90:
        return f"Excellent performance! You've achieved {cefr_level} level. Your English proficiency is at an advanced level with strong command of complex language structures."
    elif score >= 70:
        return f"Good work! You've achieved {cefr_level} level. You demonstrate solid understanding of English with room for improvement in advanced topics."
    elif score >= 40:
        return f"Fair performance. You've achieved {cefr_level} level. Continue practicing to improve your vocabulary and grammar skills."
    else:
        return f"You've achieved {cefr_level} level. Focus on building foundational vocabulary and basic grammar structures. Keep practicing!"

def generate_ai_question(topic: str, difficulty: str) -> GeneratedQuestion:
    """Generate a grammar question using AI logic."""
    import json
    import time

    # Hypothesis A: Generate AI question called
    log_entry = {
        "id": f"log_{int(time.time()*1000)}_ai_generate_single",
        "timestamp": int(time.time()*1000),
        "location": "ai-service/main.py:77",
        "message": "Generate AI question called",
        "data": {"topic": topic, "difficulty": difficulty},
        "sessionId": "debug-session",
        "runId": "admin-ai-test",
        "hypothesisId": "A"
    }

    with open('c:\\Users\\Burkay Çakar\\Desktop\\SoftwareProject\\.cursor\\debug.log', 'a') as f:
        f.write(json.dumps(log_entry) + '\n')

    # Topic-based question templates
    templates = {
        "grammar": {
            "beginner": [
                {
                    "template": "___ you ___ English? (Present Simple)",
                    "options": ["Do/study", "Does/study", "Are/studying", "Is/studying"],
                    "correct": "Do/study",
                    "explanation": "Present Simple: Do/Does + base verb for questions."
                },
                {
                    "template": "She ___ to school every day. (Present Simple)",
                    "options": ["go", "goes", "going", "gone"],
                    "correct": "goes",
                    "explanation": "Third person singular adds -s in Present Simple."
                }
            ],
            "intermediate": [
                {
                    "template": "If it ___ tomorrow, we ___ stay home. (First Conditional)",
                    "options": ["rains/will", "rain/will", "rains/would", "will rain/stay"],
                    "correct": "rains/will",
                    "explanation": "First Conditional: If + Present Simple, will + base verb."
                },
                {
                    "template": "I wish I ___ taller. (Second Conditional)",
                    "options": ["am", "were", "was", "be"],
                    "correct": "were",
                    "explanation": "Second Conditional for unreal present: wish + were."
                }
            ],
            "advanced": [
                {
                    "template": "Had I known about the party, I ___ earlier. (Third Conditional)",
                    "options": ["would come", "would have come", "came", "come"],
                    "correct": "would have come",
                    "explanation": "Third Conditional: had + past participle, would have + past participle."
                },
                {
                    "template": "The report ___ by tomorrow or we'll face consequences. (Passive Future)",
                    "options": ["must finish", "must be finished", "must have finished", "must be finishing"],
                    "correct": "must be finished",
                    "explanation": "Future Passive: will be + past participle, or modal + be + past participle."
                }
            ]
        },
        "vocabulary": {
            "intermediate": [
                {
                    "template": "The scientist made a groundbreaking ___ in renewable energy.",
                    "options": ["discovery", "invention", "finding", "creation"],
                    "correct": "discovery",
                    "explanation": "'Discovery' refers to finding something new through research."
                }
            ],
            "advanced": [
                {
                    "template": "The CEO's ___ approach to management increased productivity significantly.",
                    "options": ["authoritarian", "laissez-faire", "pragmatic", "dogmatic"],
                    "correct": "pragmatic",
                    "explanation": "'Pragmatic' means practical and realistic in approach."
                }
            ]
        }
    }

    # Get questions for the requested topic and difficulty
    topic_questions = templates.get(topic, templates["grammar"])
    difficulty_questions = topic_questions.get(difficulty, topic_questions.get("intermediate", []))

    if not difficulty_questions:
        # Fallback to basic grammar
        difficulty_questions = templates["grammar"]["intermediate"]

    # Randomly select a question
    import random
    selected = random.choice(difficulty_questions)

    # Hypothesis A: Question template selected
    log_entry = {
        "id": f"log_{int(time.time()*1000)}_ai_template_selected",
        "timestamp": int(time.time()*1000),
        "location": "ai-service/main.py:315",
        "message": "Question template selected",
        "data": {"template": selected["template"], "correct": selected["correct"]},
        "sessionId": "debug-session",
        "runId": "admin-ai-test",
        "hypothesisId": "A"
    }

    with open('c:\\Users\\Burkay Çakar\\Desktop\\SoftwareProject\\.cursor\\debug.log', 'a') as f:
        f.write(json.dumps(log_entry) + '\n')

    return GeneratedQuestion(
        text=selected["template"],
        options=selected["options"],
        correct=selected["correct"],
        explanation=selected["explanation"]
    )

@app.get("/health")
async def health_check():
    """Health check endpoint."""
    return {"status": "ok", "service": "ai-service"}

@app.post("/generate-question", response_model=GeneratedQuestion)
async def generate_question(request: QuestionRequest):
    """Generate a single grammar question based on topic and difficulty."""
    return generate_ai_question(request.topic, request.difficulty)

@app.post("/generate-questions", response_model=List[GeneratedQuestion])
async def generate_questions(request: QuestionRequest):
    """Generate multiple grammar questions."""
    import json
    import time

    # Hypothesis A: AI generate questions endpoint called
    log_entry = {
        "id": f"log_{int(time.time()*1000)}_ai_generate_request",
        "timestamp": int(time.time()*1000),
        "location": "ai-service/main.py:203",
        "message": "AI generate questions endpoint called",
        "data": {"topic": request.topic, "difficulty": request.difficulty, "count": request.count},
        "sessionId": "debug-session",
        "runId": "admin-ai-test",
        "hypothesisId": "A"
    }

    with open('c:\\Users\\Burkay Çakar\\Desktop\\SoftwareProject\\.cursor\\debug.log', 'a') as f:
        f.write(json.dumps(log_entry) + '\n')

    questions = []
    for i in range(min(request.count, 10)):  # Max 10 questions at once
        question = generate_ai_question(request.topic, request.difficulty)
        questions.append(question)

        # Hypothesis A: Individual question generated
        log_entry = {
            "id": f"log_{int(time.time()*1000)}_ai_question_generated",
            "timestamp": int(time.time()*1000),
            "location": "ai-service/main.py:225",
            "message": f"Question {i+1} generated",
            "data": {"question_number": i+1, "question_text": question.text[:50], "correct_answer": question.correct},
            "sessionId": "debug-session",
            "runId": "admin-ai-test",
            "hypothesisId": "A"
        }

        with open('c:\\Users\\Burkay Çakar\\Desktop\\SoftwareProject\\.cursor\\debug.log', 'a') as f:
            f.write(json.dumps(log_entry) + '\n')

    # Hypothesis A: All questions generated
    log_entry = {
        "id": f"log_{int(time.time()*1000)}_ai_questions_complete",
        "timestamp": int(time.time()*1000),
        "location": "ai-service/main.py:240",
        "message": "All questions generated successfully",
        "data": {"total_questions": len(questions)},
        "sessionId": "debug-session",
        "runId": "admin-ai-test",
        "hypothesisId": "A"
    }

    with open('c:\\Users\\Burkay Çakar\\Desktop\\SoftwareProject\\.cursor\\debug.log', 'a') as f:
        f.write(json.dumps(log_entry) + '\n')

    return questions

@app.post("/score", response_model=ScoreResponse)
async def score_assessment(request: ScoreRequest):
    """
    Score the English proficiency assessment.

    Zero Score Rule: If payload is empty or user answered nothing, return Score: 0, Level: A1.
    """
    import json
    import time

    # Hypothesis A: AI score endpoint entry
    log_entry = {
        "id": f"log_{int(time.time()*1000)}_ai_score_entry",
        "timestamp": int(time.time()*1000),
        "location": "ai-service/main.py:283",
        "message": "AI score endpoint called",
        "data": {
            "mcq_count": len(request.mcq_answers) if request.mcq_answers else 0,
            "writing_length": len(request.writing_text) if request.writing_text else 0,
            "speaking_length": len(request.speaking_text) if request.speaking_text else 0,
            "speaking_score": request.speaking_score
        },
        "sessionId": "debug-session",
        "runId": "exam-submit-test",
        "hypothesisId": "A"
    }

    with open('c:\\Users\\Burkay Çakar\\Desktop\\SoftwareProject\\.cursor\\debug.log', 'a') as f:
        f.write(json.dumps(log_entry) + '\n')

    # Check if payload is empty
    is_empty = (
        (not request.mcq_answers or len(request.mcq_answers) == 0) and
        (not request.writing_text or len(request.writing_text.strip()) == 0) and
        (not request.speaking_text or len(request.speaking_text.strip()) == 0)
    )

    if is_empty:
        # Hypothesis E: Empty payload detected
        log_entry = {
            "id": f"log_{int(time.time()*1000)}_ai_empty_payload",
            "timestamp": int(time.time()*1000),
            "location": "ai-service/main.py:309",
            "message": "Empty payload detected",
            "data": {"returning_zero_score": True},
            "sessionId": "debug-session",
            "runId": "initial",
            "hypothesisId": "E"
        }

        with open('c:\\Users\\Burkay Çakar\\Desktop\\SoftwareProject\\.cursor\\debug.log', 'a') as f:
            f.write(json.dumps(log_entry) + '\n')

        return ScoreResponse(
            overall_score=0,
            cefr_level="A1",
            feedback="No answers provided. Please complete the assessment to receive a score."
        )

    # Calculate individual scores
    mcq_score = calculate_mcq_score(request.mcq_answers)
    writing_score = calculate_writing_score(request.writing_text)
    speaking_score = calculate_speaking_score(request.speaking_text, request.speaking_score)

    # Hypothesis E: Individual scores calculated
    log_entry = {
        "id": f"log_{int(time.time()*1000)}_ai_individual_scores",
        "timestamp": int(time.time()*1000),
        "location": "ai-service/main.py:337",
        "message": "Individual scores calculated",
        "data": {"mcq_score": mcq_score, "writing_score": writing_score, "speaking_score": speaking_score},
        "sessionId": "debug-session",
        "runId": "initial",
        "hypothesisId": "E"
    }

    with open('c:\\Users\\Burkay Çakar\\Desktop\\SoftwareProject\\.cursor\\debug.log', 'a') as f:
        f.write(json.dumps(log_entry) + '\n')

    # Calculate weighted overall score
    # MCQ: 40%, Writing: 35%, Speaking: 25%
    overall_score = int(
        (mcq_score * 0.40) +
        (writing_score * 0.35) +
        (speaking_score * 0.25)
    )

    # Hypothesis E: Overall score calculated
    log_entry = {
        "id": f"log_{int(time.time()*1000)}_ai_overall_score",
        "timestamp": int(time.time()*1000),
        "location": "ai-service/main.py:358",
        "message": "Overall score calculated",
        "data": {
            "mcq_weighted": mcq_score * 0.40,
            "writing_weighted": writing_score * 0.35,
            "speaking_weighted": speaking_score * 0.25,
            "overall_score": overall_score
        },
        "sessionId": "debug-session",
        "runId": "initial",
        "hypothesisId": "E"
    }

    with open('c:\\Users\\Burkay Çakar\\Desktop\\SoftwareProject\\.cursor\\debug.log', 'a') as f:
        f.write(json.dumps(log_entry) + '\n')

    # Determine CEFR level
    cefr_level = determine_cefr_level(overall_score)

    # Generate feedback
    feedback = generate_feedback(overall_score, cefr_level)

    # Individual scores for debugging and correct response parsing
    individual_scores = {
        "mcq_score": mcq_score,
        "writing_score": writing_score,
        "speaking_score": speaking_score
    }

    # Hypothesis E: Final response prepared
    log_entry = {
        "id": f"log_{int(time.time()*1000)}_ai_final_response",
        "timestamp": int(time.time()*1000),
        "location": "ai-service/main.py:382",
        "message": "Final response prepared",
        "data": {"overall_score": overall_score, "cefr_level": cefr_level, "individual_scores": individual_scores},
        "sessionId": "debug-session",
        "runId": "initial",
        "hypothesisId": "E"
    }

    with open('c:\\Users\\Burkay Çakar\\Desktop\\SoftwareProject\\.cursor\\debug.log', 'a') as f:
        f.write(json.dumps(log_entry) + '\n')

    return ScoreResponse(
        overall_score=overall_score,
        cefr_level=cefr_level,
        feedback=feedback,
        individual_scores=individual_scores
    )

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8001)
